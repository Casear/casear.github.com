<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>ØMQ | IF NOT</title>
  <meta name="author" content="Casear Chu">
  
  <meta name="description" content="embeddable networking library but acts like a concurrency framework
transports 
  in-process
  inter-process
  TCP
  multicast
N-to-N with patterns 
  fanout
  pub-sub
  task distribution
  request-reply.
asynchronous I/O model
LGPLv3
The Ø in ØMQ is all about tradeoffs
zero administration, zero cost, zero waste
request and response 
沒有response 就沒辦法在send
The REQ-REP socket pair is in lockstep.
  The Lazy Pirate pattern: reliable request-reply from the client side
  The Simple Pirate pattern: reliable request-reply using load balancing
  The Paranoid Pirate pattern: reliable request-reply with heartbeating
  The Majordomo pattern: service-oriented reliable queuing
  The Titanic pattern: disk-based/disconnected reliable queuing
  The Binary Star pattern: primary-backup server failover
  The Freelance pattern: brokerless reliable request-reply
Pub-Sub
  送回來的指令看開頭來決定是否接收
  若subscribe &amp;#39;test&amp;#39; , send &amp;#39;test 1234&amp;#39;,&amp;#39;test1 1234&amp;#39;都會收到
  A subscriber can connect to more than one publisher, using one connect call each time. Data will then arrive and be interleaved (&quot;fair-queued&quot;) so that no single publisher drowns out the others.
  If a publisher has no connected subscribers, then it will simply drop all messages.
  If you&amp;#39;re using TCP and a subscriber is slow, messages will queue up on the publisher. We&amp;#39;ll look at how to protect publishers against this using the &quot;high-water mark&quot; later.
  From ØMQ v3.x, filtering happens at the publisher side when using a connected protocol (tcp: or ipc:). Using the epgm:// protocol, filtering happens at the subscriber side. In ØMQ v2.x, all filtering happened at the subscriber side.
Push-Pull
  1 worker: total elapsed time: 5034 msecs.
  2 workers: total elapsed time: 2421 msecs.
  4 workers: total elapsed time: 1018 msecs.
  The ventilator&amp;#39;s PUSH socket distributes tasks to workers (assuming they are all connected before the batch starts going out) evenly. This is called load balancing and it&amp;#39;s something we&amp;#39;ll look at again in more detail.
  The sink&amp;#39;s PULL socket collects results from workers evenly. This is called fair-queuing.
Multithread
   do not try to use the same socket from multiple threads
   you need to shut down each socket that has ongoing requests. The proper way is to set a low LINGER value (1 second),
Why We needed ZMQ
  How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision. Blocking I/O creates architectures that do not scale well. But background I/O can be very hard to do right.
  How do we handle dynamic components, i.e., pieces that go away temporarily? Do we formally split components into &quot;clients&quot; and &quot;servers&quot; and mandate that servers cannot disappear? What then if we want to connect servers to servers? Do we try to reconnect every few seconds?
  How do we represent a message on the wire? How do we frame data so it&amp;#39;s easy to write and read, safe from buffer overflows, efficient for small messages, yet adequate for the very largest videos of dancing cats wearing party hats?
  How do we handle messages that we can&amp;#39;t deliver immediately? Particularly, if we&amp;#39;re waiting for a component to come back online? Do we discard messages, put them into a database, or into a memory queue?
  Where do we store message queues? What happens if the component reading from a queue is very slow and causes our queues to build up? What&amp;#39;s our strategy then?
  How do we handle lost messages? Do we wait for fresh data, request a resend, or do we build some kind of reliability layer that ensures messages cannot be lost? What if that layer itself crashes?
  What if we need to use a different network transport. Say, multicast instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or is the transport abstracted in some layer?
  How do we route messages? Can we send the same message to multiple peers? Can we send replies back to an original requester?
  How do we write an API for another language? Do we re-implement a wire-level protocol or do we repackage a library? If the former, how can we guarantee efficient and stable stacks? If the latter, how can we guarantee interoperability?
  How do we represent data so that it can be read between different architectures? Do we enforce a particular encoding for data types? How far is this the job of the messaging system rather than a higher layer?
  How do we handle network errors? Do we wait and retry, ignore them silently, or abort?">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="ØMQ"/>
  <meta property="og:site_name" content="IF NOT"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="IF NOT" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">IF NOT</a></h1>
  <h2><a href="/">Make Things Fun</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-24T02:36:37.000Z"><a href="/2013/04/24/ømq/">Apr 24 2013</a></time>
      
      
  
    <h1 class="title">ØMQ</h1>
  

    </header>
    <div class="entry">
      
        <p>embeddable networking library but acts like a concurrency framework
transports 
  in-process
  inter-process
  TCP
  multicast
N-to-N with patterns 
  fanout
  pub-sub
  task distribution
  request-reply.
asynchronous I/O model</p>
<p>LGPLv3</p>
<p>The Ø in ØMQ is all about tradeoffs
zero administration, zero cost, zero waste</p>
<p>request and response 
沒有response 就沒辦法在send
The REQ-REP socket pair is in lockstep.
  The Lazy Pirate pattern: reliable request-reply from the client side
  The Simple Pirate pattern: reliable request-reply using load balancing
  The Paranoid Pirate pattern: reliable request-reply with heartbeating
  The Majordomo pattern: service-oriented reliable queuing
  The Titanic pattern: disk-based/disconnected reliable queuing
  The Binary Star pattern: primary-backup server failover
  The Freelance pattern: brokerless reliable request-reply</p>
<p>Pub-Sub
  送回來的指令看開頭來決定是否接收
  若subscribe &#39;test&#39; , send &#39;test 1234&#39;,&#39;test1 1234&#39;都會收到</p>
<p>  A subscriber can connect to more than one publisher, using one connect call each time. Data will then arrive and be interleaved (&quot;fair-queued&quot;) so that no single publisher drowns out the others.
  If a publisher has no connected subscribers, then it will simply drop all messages.
  If you&#39;re using TCP and a subscriber is slow, messages will queue up on the publisher. We&#39;ll look at how to protect publishers against this using the &quot;high-water mark&quot; later.
  From ØMQ v3.x, filtering happens at the publisher side when using a connected protocol (tcp: or ipc:). Using the epgm:// protocol, filtering happens at the subscriber side. In ØMQ v2.x, all filtering happened at the subscriber side.</p>
<p>Push-Pull
  1 worker: total elapsed time: 5034 msecs.
  2 workers: total elapsed time: 2421 msecs.
  4 workers: total elapsed time: 1018 msecs.
  The ventilator&#39;s PUSH socket distributes tasks to workers (assuming they are all connected before the batch starts going out) evenly. This is called load balancing and it&#39;s something we&#39;ll look at again in more detail.
  The sink&#39;s PULL socket collects results from workers evenly. This is called fair-queuing.</p>
<p>Multithread
   do not try to use the same socket from multiple threads
   you need to shut down each socket that has ongoing requests. The proper way is to set a low LINGER value (1 second),</p>
<p>Why We needed ZMQ
  How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision. Blocking I/O creates architectures that do not scale well. But background I/O can be very hard to do right.
  How do we handle dynamic components, i.e., pieces that go away temporarily? Do we formally split components into &quot;clients&quot; and &quot;servers&quot; and mandate that servers cannot disappear? What then if we want to connect servers to servers? Do we try to reconnect every few seconds?
  How do we represent a message on the wire? How do we frame data so it&#39;s easy to write and read, safe from buffer overflows, efficient for small messages, yet adequate for the very largest videos of dancing cats wearing party hats?
  How do we handle messages that we can&#39;t deliver immediately? Particularly, if we&#39;re waiting for a component to come back online? Do we discard messages, put them into a database, or into a memory queue?
  Where do we store message queues? What happens if the component reading from a queue is very slow and causes our queues to build up? What&#39;s our strategy then?
  How do we handle lost messages? Do we wait for fresh data, request a resend, or do we build some kind of reliability layer that ensures messages cannot be lost? What if that layer itself crashes?
  What if we need to use a different network transport. Say, multicast instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or is the transport abstracted in some layer?
  How do we route messages? Can we send the same message to multiple peers? Can we send replies back to an original requester?
  How do we write an API for another language? Do we re-implement a wire-level protocol or do we repackage a library? If the former, how can we guarantee efficient and stable stacks? If the latter, how can we guarantee interoperability?
  How do we represent data so that it can be read between different architectures? Do we enforce a particular encoding for data types? How far is this the job of the messaging system rather than a higher layer?
  How do we handle network errors? Do we wait and retry, ignore them silently, or abort?</p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="http://s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="http://google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:casear.chuto.tw">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/CentOS/">CentOS</a><small>1</small></li>
  
    <li><a href="/tags/CoffeeScript/">CoffeeScript</a><small>2</small></li>
  
    <li><a href="/tags/Design-Pattern/">Design Pattern</a><small>2</small></li>
  
    <li><a href="/tags/Editor/">Editor</a><small>1</small></li>
  
    <li><a href="/tags/Food/">Food</a><small>1</small></li>
  
    <li><a href="/tags/France/">France</a><small>2</small></li>
  
    <li><a href="/tags/Honeymoon/">Honeymoon</a><small>2</small></li>
  
    <li><a href="/tags/Javascript/">Javascript</a><small>3</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>3</small></li>
  
    <li><a href="/tags/Node.js/">Node.js</a><small>1</small></li>
  
    <li><a href="/tags/Technology/">Technology</a><small>1</small></li>
  
    <li><a href="/tags/Travel/">Travel</a><small>5</small></li>
  
    <li><a href="/tags/Web-Design/">Web Design</a><small>1</small></li>
  
    <li><a href="/tags/jQuery/">jQuery</a><small>1</small></li>
  
  </ul>
</div>


  

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2013 Casear Chu
  
</div>
<div class="clearfix"></div></footer>
  <script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'casear';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>